{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YLyiTwPfVCT"
      },
      "source": [
        "\n",
        "##### Copyright 2019 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Bvp6GWqtfVCW"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 Google LLC. All Rights Reserved.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JndnmDMp66FL"
      },
      "source": [
        "# DDSP Timbre Transfer Demo\n",
        "\n",
        "This notebook is a demo of timbre transfer using DDSP (Differentiable Digital Signal Processing). \n",
        "The model here is trained to generate audio conditioned on a time series of fundamental frequency and loudness. \n",
        "\n",
        "* [DDSP ICLR paper](https://openreview.net/forum?id=B1x1ma4tDr)\n",
        "* [Audio Examples](http://goo.gl/magenta/ddsp-examples) \n",
        "\n",
        "\u003cimg src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP Autoencoder figure\" width=\"700\"\u003e\n",
        "\n",
        "\n",
        "# Environment Setup\n",
        "\n",
        "\n",
        "This notebook extracts these features from input audio (either uploaded files, or recorded from the microphone) and resynthesizes with the model.\n",
        "\n",
        "Have fun! And please feel free to hack this notebook to make your own creative interactions.\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime \u003e\u003e Change Runtime Type \u003e\u003e GPU__\n",
        "* Press the ▶️button on the left of each of the cells\n",
        "* View the code: Double-click any of the cells\n",
        "* Hide the code: Double click the right side of the cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "6wZde6CBya9k"
      },
      "outputs": [],
      "source": [
        "#@title #Install\n",
        "\n",
        "#@markdown Install ddsp, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
        "\n",
        "!gcloud auth login\n",
        "!mkdir /content/repos\n",
        "!gcloud source repos clone --project=brain-magenta ddsp /content/repos/ddsp\n",
        "!pip install -Ue /content/repos/ddsp\n",
        "\n",
        "print('Copying checkpoint from GCS...')\n",
        "!mkdir /content/ckpts\n",
        "!mkdir /content/recordings\n",
        "!mkdir /content/samples\n",
        "!gsutil -q -m cp gs://magentadata/models/ddsp/solo_violin_ckpt.zip /content/ckpts/\n",
        "!unzip -o /content/ckpts/solo_violin_ckpt.zip -d /content/ckpts \u0026\u003e/dev/null\n",
        "print('Checkpoint copied!')\n",
        "\n",
        "CKPT_DIR = '/content/ckpts/solo_violin_ckpt'\n",
        "SAMPLES_DIR = '/content/samples'\n",
        "RECORDINGS_DIR = '/content/recordings'\n",
        "DEFAULT_SAMPLE_RATE = 16000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "eWSC2X9prDz_"
      },
      "outputs": [],
      "source": [
        "#@title #Import\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import librosa\n",
        "\n",
        "import gin\n",
        "import os\n",
        "import time\n",
        "import crepe\n",
        "\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab.colab_utils import (download, play, record, specplot, upload,\n",
        "                                    DEFAULT_SAMPLE_RATE)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Helper Functions\n",
        "tf.disable_v2_behavior()\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
        "f32 = ddsp.core.f32\n",
        "\n",
        "def reset_crepe():\n",
        "  \"\"\"Reset the global state of CREPE to force model re-building.\"\"\"\n",
        "  for k in crepe.core.models:\n",
        "    crepe.core.models[k] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "Go36QW9AS_CD"
      },
      "outputs": [],
      "source": [
        "#@title #Record or Upload Audio\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "\n",
        "record_or_upload = \"Upload (.mp3 or .wav)\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =  5.5 #@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = record(seconds=record_seconds)\n",
        "else:\n",
        "  # Load audio sample here (.mp3 or .wav3 file)\n",
        "  # Just use the first file.\n",
        "  filenames, audios = upload()\n",
        "  audio = audios[0]\n",
        "\n",
        "# Plot.\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "\n",
        "# Setup the session.\n",
        "tf.reset_default_graph()\n",
        "target = ''\n",
        "sess = tf.Session(target)\n",
        "tf.keras.backend.set_session(sess)\n",
        "reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "print('\\nExtracting audio features...')\n",
        "start_time = time.time()\n",
        "audio_features = ddsp.training.eval_util.compute_audio_features(audio)\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness'])\n",
        "ax[0].set_ylabel('loudness')\n",
        "\n",
        "ax[1].plot(127.0 * audio_features['f0'])\n",
        "ax[1].set_ylabel('f0 [midi note]')\n",
        "\n",
        "ax[2].plot(audio_features['f0_confidence'])\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "uQFUlIJ_5r36"
      },
      "outputs": [],
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "def mask_by_confidence(audio_features, confidence_level=0.1):\n",
        "  \"\"\"For the violin model, the masking causes fast dips in loudness. \n",
        "  This quick transient is interpreted by the model as the \"plunk\" sound.\n",
        "  \"\"\"\n",
        "  mask_idx = audio_features['f0_confidence'] \u003c confidence_level\n",
        "  audio_features['f0'][mask_idx] = 0.0\n",
        "  audio_features['loudness'][mask_idx] = -1.5\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, shift_octaves=1.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  octave_offset = 12.0 / 127.0\n",
        "  offset = shift_octaves * octave_offset\n",
        "  audio_features['f0'] += offset\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def smooth_loudness(audio_features, filter_size=3):\n",
        "  \"\"\"Smooth loudness with a box filter.\"\"\"\n",
        "  smoothing_filter = np.ones([filter_size]) / float(filter_size)\n",
        "  audio_features['loudness'] = np.convolve(audio_features['loudness'], \n",
        "                                           smoothing_filter, \n",
        "                                           mode='same')\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "# audio_features = shift_f0(audio_features, shift_octaves=1.0)\n",
        "# audio_features = mask_by_confidence(audio_features, 0.4)\n",
        "\n",
        "### Plot\n",
        "plt.figure()\n",
        "plt.plot(audio_features['f0'])\n",
        "plt.title(\"f0\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(audio_features['loudness'])\n",
        "plt.title(\"loudness\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "wmSGDWM5yyjm"
      },
      "outputs": [],
      "source": [
        "#@title #Choose a model\n",
        "\n",
        "model = 'Violin' #@param ['Violin', 'Upload your own (checkpoint folder as .zip)']\n",
        "\n",
        "\n",
        "# Parse gin config\n",
        "with gin.unlock_config():\n",
        "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "  gin.parse_config_file(gin_file, skip_unknown=False)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "n_samples_train = gin.query_parameter('additive/Additive.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio.shape[0] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "print(\"TIME_STEPS_TRAIN\", time_steps_train)\n",
        "print(\"N_SAMPLES_TRAIN\", n_samples_train)\n",
        "print(\"HOP_SIZE\", hop_size)\n",
        "print(\"TIME_STEPS\", time_steps)\n",
        "print(\"N_SAMPLES_TRANSFER\", n_samples)\n",
        "\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0', 'f0_confidence', 'loudness']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:n_samples]\n",
        "\n",
        "\n",
        "gin_params = [\n",
        "    'additive/Additive.n_samples = {}'.format(n_samples),\n",
        "    'noise/FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params)\n",
        "\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "tf.reset_default_graph()\n",
        "\n",
        "features_tf = {k:f32(v)[tf.newaxis, :] for k, v in audio_features.items()}\n",
        "\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "predictions = model.get_outputs(features_tf, training=False)\n",
        "\n",
        "target = ''\n",
        "sess = tf.Session(target)\n",
        "\n",
        "start_time = time.time()\n",
        "model.restore(sess, model_dir)\n",
        "print('Loading model took %.1f seconds' % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "SLwg1WkHCXQO"
      },
      "outputs": [],
      "source": [
        "#@title #Resynthesize Audio\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "audio_gen = sess.run(predictions['audio_gen'])[0]\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(audio_gen)\n",
        "plt.title(\"Resynthesis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgYf8dU6JdmD"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-nmIEhysGhvK"
      },
      "outputs": [],
      "source": [
        "# Adding some reverb after the fact...\n",
        "gain = 0.0\n",
        "decay = 3.0\n",
        "reverb = ddsp.effects.ExpDecayReverb(reverb_length=reverb_length)\n",
        "audio2 = sess.run(reverb(audio[np.newaxis, :], gain, decay))[0]\n",
        "play(audio2 / audio2.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4Jwf3Z1cjioV"
      },
      "outputs": [],
      "source": [
        "# Get violin stats\n",
        "tf.reset_default_graph()\n",
        "data_provider = ddsp.training.data.SoloViolin()\n",
        "batch = data_provider.get_batch(batch_size=1, shuffle=False)\n",
        "batch_np = next(tfds.as_numpy(batch))\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def rms(audio):\n",
        "  return np.mean(audio**2.0)**0.5\n",
        "  \n",
        "def normalize_by_rms(audio, norm_rms_value):\n",
        "  audio_rms = rms(audio)\n",
        "  return audio / audio_rms * norm_rms_value\n",
        "\n",
        "VIOLIN_RMS = rms(batch_np['audio'])\n",
        "norm_rms = partial(normalize_by_rms, norm_rms_value=VIOLIN_RMS)\n",
        "\n",
        "VIOLIN_RMS = 0.09"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GrgDeMZ9yTfN"
      },
      "source": [
        "## add reverb (impulse response extracted from graph above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_2oXpy1mtF61"
      },
      "outputs": [],
      "source": [
        "def add_reverb(audio):\n",
        "  # Add reverb \n",
        "  # Normalize input audio volume as well\n",
        "  tf.reset_default_graph()\n",
        "  sess = tf.Session()\n",
        "\n",
        "  dry_audio = audio.copy()\n",
        "  ir_mod = ir.copy()\n",
        "\n",
        "  PRE_RMS = rms(audio)\n",
        "  dry_audio = norm_rms(audio, norm_rms_value=AUDIO_RMS_DRY)\n",
        "  wet_audio = sess.run(ddsp.core.fft_convolve(f32(dry_audio)[tf.newaxis, :],\n",
        "                                        f32(ir_mod)[tf.newaxis, :], \n",
        "                                        delay_compensation=0))[0]\n",
        "  audio = audio + wet_audio * 1.0\n",
        "  return audio\n",
        "\n",
        "# AUDIO_RMS = rms(audio)\n",
        "# play(audio)  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3YLyiTwPfVCT",
        "JndnmDMp66FL",
        "hgYf8dU6JdmD",
        "GrgDeMZ9yTfN"
      ],
      "last_runtime": {
        "build_target": "//third_party/py/ddsp/colab:colab_notebook",
        "kind": "shared"
      },
      "name": "ddsp_timbre_transfer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
