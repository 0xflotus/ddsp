# -*-Python-*-
include 'third_party/py/ddsp/training/gin/experiments/iclr2020/nsynth_ae.gin'

# =======
# Network
# =======

# Encoder
encoders.MfccTimeDistributedRnnEncoder.z_time_steps = 250

# F0 encoder
encoders.MfccTimeDistributedRnnEncoder.f0_encoder = @encoders.ResnetF0Encoder()
encoders.ResnetF0Encoder.size = 'small'
encoders.ResnetF0Encoder.f0_bins = 256

# Parameters from onsets and frames transcription experiments.
encoders.ResnetF0Encoder.spectral_fn = @f0_spectral/spectral_ops.calc_logmel
f0_spectral/spectral_ops.calc_logmel.lo_hz = 0.0
f0_spectral/spectral_ops.calc_logmel.hi_hz = 8000.0
f0_spectral/spectral_ops.calc_logmel.bins = 229
f0_spectral/spectral_ops.calc_logmel.fft_size = 2048
f0_spectral/spectral_ops.calc_logmel.overlap = 0.75
f0_spectral/spectral_ops.calc_logmel.pad_end = True


# ===================
# Add perceptual loss
# ===================

model.Model.losses = [
    @losses.SpectralLoss(),
    @losses.PretrainedCREPEEmbeddingLoss(),
]

# Crepe
losses.PretrainedCREPEEmbeddingLoss.name = 'crepe'
losses.PretrainedCREPEEmbeddingLoss.loss_type = 'L1'
losses.PretrainedCREPEEmbeddingLoss.weight = 0.1
losses.PretrainedCREPEEmbeddingLoss.activation_layer = 'conv5-maxpool'
losses.PretrainedCREPEEmbeddingLoss.model_capacity = 'tiny'
losses.PretrainedCREPEEmbeddingLoss.checkpoint = '/path/to/model-tiny.ckpt'
